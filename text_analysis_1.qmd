---
title: "Text Analysis I at IQMR"
author: Michelle Bueno VÃ¡squez
date: today

format:
  html:
    toc: true
    embed-resources: true
    code-fold: show
    link-external-newwindow: true
    fig-dpi: 300
    code-overflow: wrap
    code-tools: true
    
execute:
  warning: false
  cache: true

from: markdown+emoji 
---

<style>
/* This targets the code inside the pre tag for output */
pre:not(.sourceCode.r) code {
  color: #355bae;
  background-color: #f3ffff;
  padding: 10px;
  border-radius: 1px;
  font-size: 10px
}
</style>




:::{.callout-note}
All credit for these materials go to Professor Ben Noble, I just compiled it into a nice html format
:::


# Lab 1: Discovering SOTU

Visit the American Presidency Project Website and choose one State of the Union Address delivered orally *after* 1981, and one written State of the Union Address delivered *before* 1900.

[Download oral addresses here](https://www.presidency.ucsb.edu/documents/app-categories/spoken-addresses-and-remarks/presidential/state-the-union-addresses)

[Download oral addresses here](https://www.presidency.ucsb.edu/documents/app-categories/citations/presidential/state-the-union-written-messages)

## Some R Basics

First, `install.packages()` and then load the following libraries

```{r loading libraries}
library(quanteda)
library(quanteda.corpora)
library(stringr)
library(tidyverse)
library(tidytext)
library(stringi)
library(textdata)
library(stm)
```

::: {.callout-note}
*note, you'll need to use devtools for quanteda.copora*:

`devtools::install_github("quanteda/quanteda.corpora")`
:::

The package `quanteda.corpora` contains a `corpus` object that contains all State of the Union Addresses.

```{r total-corpora-in-quanteda}
data_corpus_sotu
```

```{r create-sou-df}

sou_df <- as_tibble(convert(data_corpus_sotu, to = 'data.frame'))
sou_df

```
You can see that our data is now structured like a spreadsheet. Columns contain metadata like the speaker, the date, and the party. The `text` column contains the raw text of all State of the Union addresses.

You can view the full text of a speech by selecting the row using indexing and pulling the text column using `$`:

```{r viewing-full-speech}
sou_df[1,]$text
```

Using the tidyverse suite of tools, you can also perform other operations on  our dataframe. For example, if you want to see the State of the Union 
# Addresses given by Lincoln, you can use the `%>%` and `filter()` commands:

```{r filtering-by-president}
sou_df %>% 
  filter(President == 'Lincoln')
```
 
You might also be interested in creating your own variables. For example, you can use `mutate()` and `if_else(condition, true, false)` to create avariable for whether a State of the Union Address was given before or after the U.S. Civil War.

```{r adding-variables}

sou_df2 <- sou_df %>% 
  mutate(
    post_war = if_else(Date > '1865-04-09', 1, 0)
    )

```


You can also use `group_by()` and `summarise()` to aggregate and summarize your data.

```{r}
sou_df2 %>% 
  group_by(post_war) %>% 
  summarise(n = n())
```

Here, we can seen 76 speeches were given before the Civil War and 165 after.

Finally, `tolower()` converts text to lower case; this is often helpful because the mix of upper and lowercase can mess with our analysis.

```{r}

sou_df_lower <- sou_df %>% 
  mutate(
    text = tolower(text)
    )

```

# Lab 2: Getting Our Words Into a Bag
Let's focus on some of the key R code we will need to use to format our corpus as a bag of words. We will do so using the package, `quanteda.`

```{r recall-corpus}

head(data_corpus_sotu)

```
This object is already formatted as a corpus, however, we can always put text into a corpus format using the `corpus` command.

### We will practice this skill using a corpus of tweets President Trump sent while in office. 

You can import that file directly from the web using the code below. I have made some changes to the column labels to comport with more conventional formatting.

```{r loading-trump-tweets}
trump_tweets <- read_csv('https://raw.githubusercontent.com/MarkHershey/CompleteTrumpTweetsArchive/master/data/realDonaldTrump_in_office.csv') %>% 
    rename(id = ID, 
           time = Time, 
           url = `Tweet URL`, 
           text = `Tweet Text`)
```
You can view the basic file structure by using the `head()` command. You can see that a key column for us will be the `text` column which contains the tweet text.

```{r head-command}

head(trump_tweets)

```



We start by converting our text to a corpus object using the `corpus()` command. If you preview this object, you'll see that it has now been  reformatted. 

```{r corpus-command}

tt_corp <- corpus(trump_tweets)

```


### Next, let's convert this into BOW format using a series of `quanteda` functions.

The `tokens` function tokenizes our corpus. The `what = 'word'` indicates that we will be using unigrams. 

```{r preprocessing-data}

tt_dfm <- tokens(tt_corp, what = 'word',

        remove_numbers = TRUE, 
        remove_punct = TRUE, 
        remove_symbols = TRUE,
        remove_separators = TRUE) %>% 
    tokens_tolower() %>%
    tokens_remove(c(stopwords("english"))) %>%
    tokens_select(min_nchar = 3) %>% 
    dfm() %>% 
    dfm_wordstem() %>% 
    dfm_trim(min_termfreq = 5, termfreq_type = 'count',
        min_docfreq = 5, docfreq_type = 'count')

```

:::{.callout-tip}
1. Here, we remove several types of tokens we do not want, including  numbers, punctuation, symbols, and separators.

    *First, ask yourself: is there any concern about removing some of these character types from our corpus?*
2. we convert all tokens to lowercase
3. we remove a set of stopwords that are standard in the quanteda package
4. we remove all words with only one or two characters (e.g., 'a', 'at')
5. we create a dfm
6. we stem words
7. we remove rare words---those appearing less than 5 times in total and
8. those that appear in fewer than five documents
:::


We can preview our dfm now after having applied all of these steps

```{r document-feature-matrix}
tt_dfm
```

We could do some keyword counting. Let's see how many times Trump references Hilary Clinton.

```{r trump-clinton-mentions}
sum(tt_dfm[,'clinton'])
```

### We could run a simple regression using our keyword. 

One hypothesis is that as we get farther away from the 2016 election, Trump is less likely to reference Clinton.

We can extract the number of references to Clinton from our dfm

```{r extracting-clinton-mentions}
clinton_ref <- convert(tt_dfm[,'clinton'], 'data.frame')[,2]
```


We can convert the time the tweet was sent into a year variable, subtract 2016 to determine how many years away we are from the election.

```{r tweet-date-variable}
years_since_election <- year(docvars(tt_dfm)$time) - 2016
```

We perform our regression and see that for every year we get farther away from the 2016 election, the number of clinton references in a Trump tweet declines by about 0.008 on average.

```{r}

summary(lm(clinton_ref ~ years_since_election))

```



We have many other options when it comes to creating our dfm. As one example:

```{r}
tt_dfm2 <- tokens(tt_corp, what = 'word',
                  remove_numbers = TRUE, 
                  remove_punct = TRUE, 
                  remove_symbols = TRUE,
                  remove_separators = TRUE) %>%
  tokens_tolower() %>%
  tokens_remove(c(stopwords("english"), 
                  'clinton', 
                  'biden')) %>%
  tokens_ngrams(n = 1:2) %>% 
  tokens_select(min_nchar = 3) %>% 
  dfm() %>% 
  dfm_trim(min_termfreq = 50, termfreq_type = 'count',
        min_docfreq = 50, docfreq_type = 'count')
```

:::{.callout-tip}
Here we:
1.  We remove a set of stopwords that are standard in the quanteda package and custom stopwords (perhaps Trump's opponents?)
2. we can include all unigrams and bigrams after removing some features and after removing stopwords we remove more words than we did previously
:::

Here, we can preview some of the features in our dfm.

```{r features-in-dfm}
head(featnames(tt_dfm2))

```


We can also count the number of Clinton references without formatting our data as a dfm using the `str_count()` command from the `stringi` package.

```{r using-str-count}

clinton_ref_df <- trump_tweets %>% 
    mutate(text = tolower(text),
        clinton_ref = str_count(text, 'clinton'),
        years_since_election = year(time) - 2016)

```


We can see the number of references we found
```{r}

table(clinton_ref_df$clinton_ref)

```

Same regression as above, and we get the same results

```{r}
summary(lm(clinton_ref ~ years_since_election, clinton_ref_df))
```


Note that `str_count()` will capture some word stems. For example, it will count both 'republican' and 'republicans' when you search for 'republican'

```{r}

example_text = 'The republican party is made up of republicans.'
str_count(example_text, 'republican') == 2

```


### Let's try to replicate Ben Noble's dictionary based sentiment analysis from lecture in this data set. 

We can access the `afinn` dictionary from the `tidytext` package.

```{r afinn-dictionary}
# This is a data frame of words and sentiment scores
afinn_dict <- get_sentiments('afinn')

# let's give each text a unique id number
clinton_ref_df$unique_id <- 1:nrow(trump_tweets)

# starting from our original data
trump_words <- clinton_ref_df %>% 
    unnest_tokens(word, text)

trump_sentment <- trump_words %>% 
    left_join(afinn_dict) %>% 
    mutate(value = if_else(is.na(value), 0, value))

trump_score_df <- trump_sentment %>% 
    group_by(id, 
             time, 
             url, 
             unique_id, 
             clinton_ref, 
             years_since_election) %>% 
    summarise(
      tot_score = sum(value), 
      word_count = n()
      ) %>% 
    mutate(
      sent_score = tot_score / word_count
      )
```



:::{.callout-tip}
Are speeches referencing clinton more negative than those not?
```{r sentiment-regression}
summary(lm(sent_score ~ clinton_ref, trump_score_df))
```


yes---speeches referencing clinton are about 0.07 points more negative!
:::

## Your Turn!

**Starting with the original df of tweets, `trump_tweets` do the following:**

**1) Create a dfm with the following characteristics:**
 - unigrams
 - remove punctuation, symbols, and separators, but _not_ numbers
 - convert tokens to lowercase
 - remove stopwords from quanteda
 - remove all letters (single character words) 
 - stem the words
 - trim the corpus so that you keep all words that appear at least 10 times
   and those that appear across at least 5 tweets

:::{.callout-tip}
```{r}
tt_corp_ex <- corpus(trump_tweets)

tt_dfm_ex <- tokens(tt_corp_ex, what = 'word',
    # here, we remove several types of tokens we do not want, including 
    # punctuation, symbols, and separators, but not numbers
        remove_numbers = FALSE, 
        remove_punct = TRUE, 
        remove_symbols = TRUE,
        remove_separators = TRUE) %>% 
    # we convert all tokens to lowercase
    tokens_tolower() %>%
    # we remove a set of stopwords that are standard in the quanteda package
    tokens_remove(c(stopwords("english"))) %>%
    # we remove all words with only one character
    tokens_select(min_nchar = 2) %>% 
    # we create a dfm
    dfm() %>% 
    # we stem words
    dfm_wordstem() %>% 
    # we remove rare words---those appearing less than 10 times in total and
    # those that appear in fewer than five tweets
    dfm_trim(min_termfreq = 10, termfreq_type = 'count',
        min_docfreq = 5, docfreq_type = 'count')

```
:::


**2) What is N, J, and the sparsity of the dfm?**

:::{.callout-tip}
N = 23,073 (the number of documents), 
J = 3,550 (the number of unique features), and the sparsity is 99.63% (the percentage of cells that are 0).
:::

**3) How often does Trump reference immigration? Let's suppose we can capture all references to immigration using the stem 'immigr' and the stem 'border'.**

:::{.callout-tip}
```{r}
# extract columns with features immigr and border, sum the totals
sum(tt_dfm_ex[,c('immigr', 'border')])
```
:::

**4) Using the original data, `trump_tweets`, create a new df where you use  `str_count()` to count instances of both 'immigr' (note, this will capture complete words like 'immigrant' and 'immigration') and 'border'. Don't forget to lowercase the text!**

*Hint: it might be easier to do this is multiple steps. First, count the number of 'immigr' references in a variable, then the number of 'border' references, and finally, add those two variables together for the final count.*

:::{.callout-tip}
```{r}
immig_df <- trump_tweets %>% 
# lowercase text
    mutate(text = tolower(text),
        # count 'immigr' references
        immigr = str_count(text, 'immigr'),
        # count 'border' references
        border = str_count(text, 'border'),
        # add both reference types into a single variable
        all_ref = immigr + border)
```
:::

**5) Using the tidytext method, convert the you just created to one where each row is a word within a document. Before you do this, create unique document ids! **

:::{.callout-tip}
```{r}
# assign unique id
immig_df$unique_id <- 1:nrow(immig_df)

immig_toks <- immig_df %>% 
    # create unigrams
    unnest_tokens(word, text)
```
:::

**6) The NRC dictionary contains a df of words and emotion codes, including anger. The code below will read in that dictionary and subset just to anger words. Merge this dictionary with the tweet data and aggregate back to the document level counting the proportion of anger words per words in tweets. **

*When you group, make sure to include the unique_id and the category that counts all immigration references. Also make sure to convert NA (non-anger) words to 0s!*

:::{.callout-tip}
```{r}
# read in nrc dictionary and filter to anger words
nrc_anger <- get_sentiments('nrc') %>% 
    filter(sentiment == 'anger')

anger_df <- immig_toks %>% 
    # merge the nrc anger dict
    left_join(nrc_anger) %>% 
    # convert NAs to 0s
    mutate(anger = if_else(is.na(sentiment), 0, 1)) %>% 
    # grouping variables
    group_by(unique_id, time, url, all_ref) %>% 
    # compute the total anger per tweet and total word count
    summarise(n_anger = sum(anger), wc = n()) %>% 
    # create the proportion of anger words in a tweet
    mutate(anger_pct = n_anger / wc)
```
:::

**7) Run a regression to test whether tweets that reference immigration more are angrier on average. What do you conclude?**

:::{.callout-tip}
```{r}
summary(lm(anger_pct ~ all_ref, anger_df))

```

For each additional immigration reference, the proportion of anger words used in a tweet is 0.5% higher.
:::

**8) Our measure of immigration references is probably imperfect. As a class, let's try to assess how it performs. Run the code below. It will create a csv file on your desktop. Read through each tweet and determine whether it is or isn't about immigration. If so, add a 1 to the column named `true_immigr`, otherwise put a 0 in this column. If you finish email me the csv that includes the codes you added.**

:::{.callout-tip}
```{r}
# randomly sample 10 immigration rows and 10 non-immigration rows
immig_val <- bind_rows(
        immig_df %>% filter(all_ref == 1) %>% sample_n(20),
        immig_df %>% filter(all_ref == 0) %>% sample_n(20)
    ) %>% 
    # create an empty column for coding
    mutate(true_immigr = NA_real_) %>% 
    # subset to relevant variables
    select(time, text, all_ref, true_immigr)

# write validation set to csv
write_csv(immig_val, 'immig_val.csv')
```
:::

# Lab 3: Using Topic Modeling Software
*note: Part I was the hard codring by hand-- Prof. Noble suggested skipping that and using the software in our own work*

## Part II

```{r}
## Part II: Using Topic Modeling Software
library(tidyverse)
library(stm)

# Let's return to Donald Trump's tweets from the last lab
trump_tweets <- read_csv('https://raw.githubusercontent.com/MarkHershey/CompleteTrumpTweetsArchive/master/data/realDonaldTrump_in_office.csv') %>% 
    rename(id = ID, time = Time, url = `Tweet URL`, text = `Tweet Text`)
trump_tweets$uid <- 1:nrow(trump_tweets)

# Using our code from last time, let's create a dfm.
tt_corp <- corpus(trump_tweets)
tt_dfm <- tokens(tt_corp, what = 'word',
        remove_numbers = TRUE, 
        remove_punct = TRUE, 
        remove_symbols = TRUE,
        remove_separators = TRUE) %>% 
    tokens_tolower() %>%
    tokens_remove(c(stopwords("english"))) %>%
    tokens_select(min_nchar = 3) %>% 
    dfm() %>% 
    dfm_wordstem() %>% 
    dfm_trim(min_termfreq = 5, termfreq_type = 'count',
        min_docfreq = 5, docfreq_type = 'count')

# due to the pre-processing, some of the documents have no tokens, let's remove
tt_omit <- tt_dfm[!rowSums(tt_dfm) == 0,]
```

This line of code runs our LDA model. This can take a while as the model runs through several iterations. The default tolerance is 1e-5, but that is going to take too long for a demo, I set it to 5e-4 but you can remove this in actual analysis.

```{r}
tt_mod <- stm(tt_omit, 
              K = 15, 
              init.type = 'LDA', 
              seed = 20221208, 
              emtol = 5e-4)
```

Once the model fits, use the following code to inspect the top words associated with each topic. What do you think each topic represents?

```{r}
labelTopics(tt_mod)
```

For one topic, use the following code to inspect some representative docs

```{r}
# First, we need to remove the "empty" documents from our corpus
tt_corp_edit <- tt_corp[tt_corp$uid %in% tt_omit$uid]

# Ensure the corpus and document-feature matrix alignment
valid_doc_ids <- docnames(tt_omit)
tt_corp_edit <- subset(tt_corp, docid(tt_corp) %in% valid_doc_ids)
tt_texts <- as.character(tt_corp_edit)
```

This code finds 3 representative docs for topic 4 (but you can change these settings)
```{r}
top4 <- findThoughts(tt_mod, 
                     tt_corp_edit, 
                     n = 3, 
                     topic = 4)
plot(top4, width = 80)
```

You can also see the topic proportions within each document.

```{r}
head(as.data.frame(tt_mod$theta))
```


```{r}
# let's create a vector of which topic is the most prevalent in each tweet
max_topic <- c()
for (i in 1:nrow(tt_mod$theta)){
  max_topic <- c(max_topic, which.max(tt_mod$theta[i,]))
}
```

The code was bugging, so here is the ChatGPT/Michelle debug:
```{r}
# Create a vector of which topic is the most prevalent in each tweet
max_topic <- apply(tt_mod$theta, 1, which.max)

# Convert the filtered corpus to a data frame
tt_corp_df <- convert(tt_corp_edit, to = "data.frame")

# Add the max_topic vector to the data frame
tt_corp_df$max_topic <- max_topic

# Extract year from the time column and add it to the data frame
tt_corp_df$year <- format(as.Date(tt_corp_df$time), "%Y")

# Print intermediate data to debug
print(head(tt_corp_df))
print(table(tt_corp_df$year))
print(table(tt_corp_df$max_topic))

# Ensure the year column exists
if (!"year" %in% colnames(tt_corp_df)) {
  stop("Year column not found in the data frame")
}

# See how often each topic is discussed by year
topic_summary <- tt_corp_df %>% 
  group_by(year, max_topic) %>% 
  summarise(n = n(), .groups = 'drop') %>% 
  arrange(year, max_topic)

print(topic_summary)

# Identify the top topic for each year based on the count (n)
top_topics_per_year <- topic_summary %>%
  group_by(year) %>%
  top_n(1, n) %>%
  arrange(year)

# Print the top topics per year
print(top_topics_per_year)

knitr::kable(top_topics_per_year)

```

Time permitting, change the settings. Try a different number of topics and see what (if anything) changes. Try to change the pre-processing and see if 15 topics is different from the original 15 topics.

